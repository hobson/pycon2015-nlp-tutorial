<!--
slidedeck: A modification of the Google IO 2012 HTML5 slide template
URL: https://github.com/rmcgibbo/slidedeck

Based on https://github.com/francescolaffi/elastic-google-io-slides, and
ultimately:

Google IO 2012 HTML5 Slide Template
Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>
URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title> Practical Natural Language Processing</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
  <!--This one seems to work all the time, but really small on ipad-->
  <!--<meta name="viewport" content="initial-scale=0.4">-->
  <meta name="apple-mobile-web-app-capable" content="yes">
   <link rel="shortcut icon" href=" http://hobson.github.io/pycon2015-nlp-tutorial/favicon.ico"/> 
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/custom.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>

  <!-- MathJax support  -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    showProcessingMessages: false,
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">
<slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">

    <h1> Practical Natural Language Processing</h1>
    <h2> PyCon 2015, Montreal<br/> <i><small>April, 2015</small></i></h2>
    <p> <a href="https://github.com/hobson">Hobson Lane</a></p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2></h2>
      <h3></h3>
    </hgroup>
    <article ><p>http://hobson.github.io/analytics-summit-2015/favicon.ico</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Audience Survey</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li>Who has done Natural Language Processing?<ul class="build">
<li>Stats?  ML?  Parsing?</li>
<li>Worked with Large Data Sets? DBs or ORMs?<ul class="build">
<li><code>postgres</code>, <code>mysql</code>, <code>neo4j</code>, <code>sqlite</code>, <code>django</code></li>
</ul>
</li>
</ul>
</li>
<li>Used other tools for NLP?<ul class="build">
<li>grep, sed, awk, cut?</li>
</ul>
</li>
<li>NLP (Natural Language Processing) using Python?<ul class="build">
<li><code>nltk</code>, <code>scrapy</code>, <code>numpy</code>, <code>fuzzywuzzy</code>, <code>collections.Counter</code></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>What is natural language</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>a system of utterances invented by humans "spontaneously" over millions of years.</li>
</ol>
</li>
<li>
<ol>
<li>unstructured text is a generalization of natural language text and the terms are often used interchangeably</li>
</ol>
</li>
<li>
<ol>
<li>natural language is often embedded in structure text (formal languages), like HTML, XML, YAML, SQL, and of course Python as the content of variables, elements, or strings</li>
</ol>
</li>
<li>
<ol>
<li>Examples (why NLP is challenging):</li>
</ol>
</li>
<li>
<ul>
<li>HTML tag contents, e.g. the "PyCon 2015..." in <title>PyCon 2015 in Montréal | April 8th – April 16th</title></li>
</ul>
</li>
<li>
<ul>
<li>A textbook, encyclopedia or Wikipedia articles with headings, page numbers, footnotes, etc</li>
</ul>
</li>
<li>
<ul>
<li>A social network feed (twitter, facebook, ), e.g. "Brushed my teeth today"</li>
</ul>
</li>
<li>
<ul>
<li>A legal contract, license agreement (EULA), annual report, patent "By checking this box you sign away your rights to sue us."</li>
</ul>
</li>
<li>
<ul>
<li>Notes to yourself: "Don't forget to take Plato for a walk"</li>
</ul>
</li>
<li>
<ul>
<li>Chat room correspondence: "OMG dont be sucha troll!!!"</li>
</ul>
</li>
<li>
<ul>
<li>Numbers and prices (e.g. "200 pythonistas", "$50K per year", "1 GB")</li>
</ul>
</li>
<li>
<ol>
<li>Nonexamples</li>
</ol>
</li>
<li>
<ul>
<li>HTML and CSS tags</li>
</ul>
</li>
<li>
<ul>
<li>python script (but some strings within it may be NL)</li>
</ul>
</li>
<li>
<ul>
<li>A CSV file (but some strings within the fields may be NL)</li>
</ul>
</li>
<li>
<ul>
<li>mathematical equations (but the integers and fractions within it can be processed as NL)</li>
</ul>
</li>
<li>
<ul>
<li>a database (but the names of the fields and tables may be processed as NL)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>What is natural language processing</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>Computer processing of languages to do something useful or fun</li>
</ol>
</li>
<li>
<ol>
<li>Story about the genesis of <em>information theory</em> and introduction to concepts Applicable to NLP:</li>
</ol>
</li>
<li>
<ul>
<li>entropy</li>
</ul>
</li>
<li>
<ul>
<li>coding</li>
</ul>
</li>
<li>
<ul>
<li>decoding</li>
</ul>
</li>
<li>
<ul>
<li>compression</li>
</ul>
</li>
<li>
<ul>
<li>encryption</li>
</ul>
</li>
<li>
<ul>
<li>channel capacity</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Why is natural language processing useful and fun</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>Example applications</li>
</ol>
</li>
<li>a. sentiment analysis of customer service data (SAP)</li>
<li>b. sentiment analysis for trend and finance prediction on twitter and other news feeds (Thomson Reuters)</li>
<li>
<ul>
<li>Reuters provides a low-latency feed to hedge funds containing a single bit associated with a stock symbol -- positive or negative impact on price</li>
</ul>
</li>
<li>c. hardware performance trends based on technician inspection comments (Sharp Electronics Corporation)</li>
<li>d. enable artificial intelligence agents to train/teach themselves (CMU's NELL)</li>
<li>e. data migration (ETL) between bodies of structured text like CSV, HTML tables to save the planet (DOE and Building Energy)</li>
<li>f. example processing of memoirs for psychoanalysis: Stephen Fry's autobiography and its <a href="http://yourfry.com/">open API + metadata</a></li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>What does artificial intelligence have to do with NLP</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>Turing defined it as being able to imitate a human's ability to converse in  natural language text</li>
</ol>
</li>
<li>
<ol>
<li>In some ways coding languages, structured text, and data structures, are just a subset or specialization of natural languages (because they are meant to be written and read by humans <em>AND</em> machines)</li>
</ol>
</li>
<li>
<ol>
<li>semantic processing (state of the art NLP) extracts knowledge or meaning from text</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Context:</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>what is context</li>
</ol>
</li>
<li>
<ol>
<li>why is it important?</li>
</ol>
</li>
<li>
<ol>
<li>Some common layers of context and meaning</li>
</ol>
</li>
<li>
<ol>
<li>word (the "meaning" of syllables depends on the word they are used in)</li>
</ol>
</li>
<li>
<ol>
<li>compound word ("boot" means something different in "bootstrap" and "boot up")</li>
</ol>
</li>
<li>
<ol>
<li>phrase (noun-phrases are particularly "atomic")</li>
</ol>
</li>
<li>
<ol>
<li>sentence (a sentence can often be presumed to have some grammatically-required elements like a noun and a verb)</li>
</ol>
</li>
<li>
<ol>
<li>paragraph (paragraphs often have an intro, body, conclusion with different word usage assumptions)</li>
</ol>
</li>
<li>
<ol>
<li>passage (quotes, excerpts)</li>
</ol>
</li>
<li>
<ol>
<li>page (text often will refer to images or quotes on the same page, like "see above")</li>
</ol>
</li>
<li>
<ol>
<li>section (topics are changed between sections of an article or book)</li>
</ol>
</li>
<li>
<ol>
<li>chapter (authors change viewpoint/location/subject between chapters)</li>
</ol>
</li>
<li>
<ol>
<li>book (terms and symbols used in a dictionary may only be relevant there)</li>
</ol>
</li>
<li>
<ol>
<li>corpus (a subset of language usages will always have sample biases)</li>
</ol>
</li>
<li>
<ol>
<li>language ("taco" means something different in English than in Spanish)</li>
</ol>
</li>
<li>
<ol>
<li>tribe/city/region ("Zoobombing" means something completely different in Portland than in a war zone)</li>
</ol>
</li>
<li>
<ol>
<li>nation (culture)</li>
</ol>
</li>
<li>
<ol>
<li>planet (yes, projects like SETI are very concerned with NLP of ET languages)</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Tools and techniques for identifying "style" or "context"</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>Sentiment and mood metrics</li>
</ol>
</li>
<li>
<ol>
<li>Style distance metrics for comparison of a text to that of famous authors like Hemmingway, Shakespeare, US presidents,</li>
</ol>
</li>
<li>
<ol>
<li>Techniques</li>
</ol>
</li>
<li>
<ul>
<li>vocabulary breadth</li>
</ul>
</li>
<li>
<ul>
<li>vocabulary statistics relative to "standard vocabularies" or age-group vocabularies from education material</li>
</ul>
</li>
<li>
<ul>
<li>vacabulary evolution over the course of a document</li>
</ul>
</li>
<li>
<ul>
<li>structure (vocabulary shifts within a document for each of the context layers discussed previously)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Break</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ul>
<li>Will help those having trouble getting the examples so far to work in their environment</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Acquiring a Corpus</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>using <code>nltk</code> to download text corpora (text documents or strings)</li>
</ol>
</li>
<li>
<ol>
<li>extracting text and semi-structure text (tables) from web pages using Scrapy and Beautiful Soup</li>
</ol>
</li>
<li>
<ol>
<li>encoding issues and python 2 vs 3: what has changed and how to use the best of python 3 with python 2 NLP tools</li>
</ol>
</li>
<li>
<ul>
<li><code>unicode()</code> vs <code>str()</code> vs <code>repr()</code></li>
</ul>
</li>
<li>
<ul>
<li><code>from __future__ import unicode_literals</code></li>
</ul>
</li>
<li>
<ol>
<li>check user-supplied corpus  with some common tools for "quantifying" and structuring unstructured text</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Frequency analysis of US President inaugural speeches</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>segmentation/tokenization/parsing</li>
</ol>
</li>
<li>
<ul>
<li>characters (encoding issues, some natural languages like Japanese Kanji and Chinese don't have "letters")</li>
</ul>
</li>
<li>
<ul>
<li>words</li>
</ul>
</li>
<li>
<ul>
<li>digits and symbols and unicode as part of words</li>
</ul>
</li>
<li>
<ul>
<li>punctuation at the end of sentences and word</li>
</ul>
</li>
<li>
<ul>
<li>hyphenation</li>
</ul>
</li>
<li>
<ul>
<li>typos</li>
</ul>
</li>
<li>
<ul>
<li>spelling variations (British English)</li>
</ul>
</li>
<li>
<ul>
<li>language variations (Spanish, French, slang)</li>
</ul>
</li>
<li>
<ul>
<li>bag-of-words counting (frequency analysis) ignores context at any layer above the "documents"</li>
</ul>
</li>
<li>
<ul>
<li>agnostic counting</li>
</ul>
</li>
<li>
<ol>
<li>stemming</li>
</ol>
</li>
<li>
<ul>
<li>nltk stemmers</li>
</ul>
</li>
<li>
<ol>
<li>counting</li>
</ol>
</li>
<li>
<ul>
<li>Data structures like <code>collections.Counter</code> that discard context/order</li>
</ul>
</li>
<li>
<ul>
<li>Can <code>collections.OrderedDict</code> be used to preserve context and order? (not easily)</li>
</ul>
</li>
<li>
<ol>
<li>normalization of counts/frequencies/probabilities</li>
</ol>
</li>
<li>
<ol>
<li>occurrence matrices ("word space" or "word vector space" in information theory)</li>
</ol>
</li>
<li>
<ul>
<li>uses for word-word, word-document, document-word, and document-document matrices</li>
</ul>
</li>
<li>
<ul>
<li>"word space" is a way of giving words a distance metric, from each other as individuals and as collections of words (documents)</li>
</ul>
</li>
<li>
<ul>
<li>Leventshtein distance</li>
</ul>
</li>
<li>
<ul>
<li>Distance</li>
</ul>
</li>
<li>
<ul>
<li>statistical (frequency) word space</li>
</ul>
</li>
<li>
<ul>
<li>nltk.metrics.distance.jaccard_distance</li>
</ul>
</li>
<li>
<ul>
<li>nltk.metrics.distance.masi_distance</li>
</ul>
</li>
<li>
<ul>
<li>nltk.metrics.distance.presence</li>
</ul>
</li>
<li>
<ul>
<li>direct semantic word space (we'll talk about WordNet later)</li>
</ul>
</li>
<li>
<ul>
<li>syntactic/gramatical word space (we'll talk about POS tagging later)</li>
</ul>
</li>
<li>
<ul>
<li>statistical nltk distance measures/metrics:</li>
</ul>
</li>
<li>
<ol>
<li>complexity/entropy/information measures for unstructured text</li>
</ol>
</li>
<li>a. compression ratio</li>
<li>b. entropy</li>
<li>c. predictability (human trials by Claude Shannon et al.)</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Dimension reduction</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>occurrence matrices will grow to become impractical</li>
</ol>
</li>
<li>
<ul>
<li>100k words/tokens counted across 10k documents = 1 GB of data, if stored efficiently</li>
</ul>
</li>
<li>
<ul>
<li>ignoring "stop words" and low-information-content words won't significantly reduce the dimensions</li>
</ul>
</li>
<li>
<ul>
<li>many machine learning algorithms are impractical at this scale:</li>
</ul>
</li>
<li>
<ul>
<li>decision trees</li>
</ul>
</li>
<li>
<ul>
<li>KNN</li>
</ul>
</li>
<li>
<ul>
<li>K-means</li>
</ul>
</li>
<li>
<ul>
<li>Support vector machines</li>
</ul>
</li>
<li>
<ul>
<li>SVD (PCA) can reduce the dimensions and enable many powerful machine learning algorithms to be employed</li>
</ul>
</li>
<li>
<ul>
<li>When SVD is impractical (e.g. 100k x 100k matrices or larger), dimension reduction can be based on the entropy found in each word and document independent of the others</li>
</ul>
</li>
<li>
<ol>
<li>ntlk US inaugural presidential speech word-frequency example</li>
</ol>
</li>
<li>
<ul>
<li>raw occurrence matrices</li>
</ul>
</li>
<li>
<ul>
<li>reduced-dimension occurrence matrices</li>
</ul>
</li>
<li>
<ol>
<li>d3 visualizations of occurrence matrices</li>
</ol>
</li>
<li>
<ul>
<li>as "checkerboard" grids or heat-maps</li>
</ul>
</li>
<li>
<ul>
<li>as graphs or networks (D3 force-directed graph)</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Getting Fuzzy</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>regular expressions</li>
</ol>
</li>
<li>
<ul>
<li>examples for use in a chat bot</li>
</ul>
</li>
<li>
<ul>
<li>examples for use in a crawler for financial information</li>
</ul>
</li>
<li>
<ul>
<li>what they're good at (semi-structured text) and what their not good for (not robust/reliable)</li>
</ul>
</li>
<li>
<ol>
<li>fuzzywuzzy (uses "quick" Levenshtein distance)</li>
</ol>
</li>
<li>
<ul>
<li>examples for matching database table/column names</li>
</ul>
</li>
<li>
<ul>
<li>when you need the "best" match and you need it fast</li>
</ul>
</li>
<li>
<ol>
<li>fuzzy regular expressions (<code>regex</code> package)</li>
</ol>
</li>
<li>
<ul>
<li>example use in a chatbot (<code>will</code>)</li>
</ul>
</li>
<li>
<ul>
<li>when you want the very "best match" and you can wait</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Break</h2>
      <h3></h3>
    </hgroup>
    <article ></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Jargon and typo consolidation</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ul>
<li>Tell a story about the need for word consolidation (vocab dimension reduction) at Sharp Electronics to interpret multilingual, jargony, and abbreviation-filled natural language repair technician notes</li>
</ul>
</li>
<li>
<ul>
<li>word distance metrics (Levenshtein)</li>
</ul>
</li>
<li>
<ul>
<li>which word is the "canonical" form among a list of typos</li>
</ul>
</li>
<li>
<ul>
<li>sorting by frequency</li>
</ul>
</li>
<li>
<ul>
<li>sorting by word length</li>
</ul>
</li>
<li>
<ul>
<li>iterative loosening of fuzziness threshold</li>
</ul>
</li>
<li>
<ul>
<li>use of dictionaries and word graphs (WordNet, ConceptNet) to find the "root" of a word</li>
</ul>
</li>
<li>
<ul>
<li>example usages o</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Parsing structured data to create small , context-specific databases of knowledge for use in NLP information extraction from unstructured text</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>extracting tabular data automatically from a web page (using Scrapy)</li>
</ol>
</li>
<li>
<ol>
<li>automatic-identification of table "schema" using frequency analysis</li>
</ol>
</li>
<li>
<ol>
<li>parsing a natural language database table query and producing a natural language response (i.e. how Wolfram Alpha does it)</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Natural language processing of search queries</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>composing logical statements from natural language</li>
</ol>
</li>
<li>
<ol>
<li>state of the art in semantic parsing of sentences</li>
</ol>
</li>
<li>
<ol>
<li>word order to tag words with POS and context</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Break</h2>
      <h3></h3>
    </hgroup>
    <article ></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Knowledge extraction</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>date/time information using python-dateutil</li>
</ol>
</li>
<li>
<ul>
<li><code>will</code> example "remind me to knock off at 5"</li>
</ul>
</li>
<li>
<ol>
<li>regexes to extract prices</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Sentiment analysis to gage chat room "mood"</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>Applications of sentiment analysis</li>
</ol>
</li>
<li>
<ul>
<li>financial analytic forecasting</li>
</ul>
</li>
<li>
<ul>
<li>social network research and traffic/mood manipulation</li>
</ul>
</li>
<li>
<ul>
<li>advertising</li>
</ul>
</li>
<li>
<ul>
<li>consumer feedback processing</li>
</ul>
</li>
<li>
<ul>
<li>movie, music, book review processing and triage</li>
</ul>
</li>
<li>
<ol>
<li>will` chatbot example using nltk</li>
</ol>
</li>
<li>
<ul>
<li>gage the mood and civility of your chat room</li>
</ul>
</li>
<li>
<ul>
<li>long term "culture" tracking</li>
</ul>
</li>
<li>
<ul>
<li>identifying the mood of individual chatroom members</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Sentence structure</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li><code>nltk</code> POS tagging tools and examples</li>
</ol>
</li>
<li>
<ol>
<li>grammar analysis and checking</li>
</ol>
</li>
<li>
<ol>
<li>using POS to aid in information/knowledge extraction</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Semantic processing</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<ol>
<li>nltk WordNet interface\</li>
</ol>
</li>
<li>
<ol>
<li>analyze and visualize the semantic network for the participant-supplied text</li>
</ol>
</li>
<li>
<ol>
<li>use NLTK to populate a simple knowledge base about you based on your hard drive contents</li>
</ol>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Metadata</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>Time: 4.4 hr</li>
<li>Bullets: 180</li>
<li>Slides: 20</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Contributors</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>Hobson Lane <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#117;&#103;&#97;&#117;&#116;&#104;&#111;&#114;&#115;&#64;&#116;&#111;&#116;&#97;&#108;&#103;&#111;&#111;&#100;&#46;&#99;&#111;&#109;">&#112;&#117;&#103;&#97;&#117;&#116;&#104;&#111;&#114;&#115;&#64;&#116;&#111;&#116;&#97;&#108;&#103;&#111;&#111;&#100;&#46;&#99;&#111;&#109;</a></li>
<li>LiZhong Zheng <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#105;&#122;&#104;&#111;&#110;&#103;&#64;&#77;&#73;&#84;&#46;&#101;&#100;&#117;">&#108;&#105;&#122;&#104;&#111;&#110;&#103;&#64;&#77;&#73;&#84;&#46;&#101;&#100;&#117;</a></li>
<li>Your Name Here ;) <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#117;&#103;&#97;&#117;&#116;&#104;&#111;&#114;&#115;&#64;&#116;&#111;&#116;&#97;&#108;&#103;&#111;&#111;&#100;&#46;&#99;&#111;&#109;">&#112;&#117;&#103;&#97;&#117;&#116;&#104;&#111;&#114;&#115;&#64;&#116;&#111;&#116;&#97;&#108;&#103;&#111;&#111;&#100;&#46;&#99;&#111;&#109;</a></li>
</ul></article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <!-- <aside class="gdbar right"><img src="images/google_developers_icon_128.png"></aside> -->
  <article class="flexbox vleft auto-fadein">
    <h2> Thank you <a href="https://www.sharplabs.com">Sharp Labs!</a></h2>
    <p> for being <strong>awesome</strong> and contributing to open source...</p>
  </article>
  <p data-config-contact class="auto-fadein"> PDX Python's <a href="https://github.com/pug/pug">pug</a><br/> Steven Skoczen's <a href="http://github.com/skoczen/will">will</a><br/> Steven Bird's <a href="https://github.com/nltk">nltk</a><br/> Mike Bostock's <a href="http://d3js.org">D3</a></p>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>

<script>
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-XXXXXXXX-1']);
_gaq.push(['_trackPageview']);

(function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<!--[if IE]>
  <script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
  <script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>